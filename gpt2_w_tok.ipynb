{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplicingSitesDataset(Dataset):\n",
    "\tdef __init__(self, sequences, labels, tokenizer, max_length):\n",
    "\t\tself.sequences = sequences\n",
    "\t\tself.labels = labels\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.max_length = max_length\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.sequences)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tprompt = self.sequences[idx]\n",
    "\t\tlabel = self.labels[idx]\n",
    "\n",
    "\t\tinput_text = f\"sequence: {prompt} awnser: \"\n",
    "\t\toutput_text = f\"{label}\"\n",
    "\t\tinput_ids = self.tokenizer.encode(input_text, truncation=True, max_length=self.max_length, add_special_tokens=True, padding=True)\n",
    "\t\tlabel_ids = self.tokenizer.encode(output_text, truncation=True, max_length=self.max_length, add_special_tokens=False)\n",
    "\n",
    "\t\tinput_ids += label_ids\n",
    "\t\tlabels = [-100] * len(input_ids[:-len(label_ids)]) + label_ids\n",
    "\n",
    "\t\treturn torch.tensor(input_ids), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\tinput_ids, labels = zip(*batch)\n",
    "\tmax_len = max(len(ids) for ids in input_ids)\n",
    "\tinput_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\tlabels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\treturn input_ids_padded, labels_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, epochs=3, device=\"cuda\"):\n",
    "\tmodel.train()\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor batch in dataloader:\n",
    "\t\t\tinput_ids, labels = [b.to(device) for b in batch]\n",
    "\t\t\toutputs = model(input_ids=input_ids, labels=labels)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\tprint(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, sequence, device=\"cuda\"):\n",
    "\tmodel.eval()\n",
    "\tinput_text = f\"sequence: {sequence} awnser: \"\n",
    "\tinput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = model.generate(\n",
    "\t\t\tinput_ids,\n",
    "\t\t\tmax_new_tokens=10,\n",
    "\t\t\trepetition_penalty=2.0,\n",
    "\t\t\ttop_k=50,\n",
    "\t\t\ttop_p=0.9,\n",
    "\t\t\tpad_token_id=tokenizer.eos_token_id,\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tcompletion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\t\treturn completion.replace(input_text, \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./database/col_ac.mod1\", \"rb\")\n",
    "data = pickle.load(file)\n",
    "\n",
    "database = data[\"train\"] + data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "introns_data = []\n",
    "exons_data = []\n",
    "\n",
    "for sequence in database:\n",
    "\tintrons = sequence[\"introns\"]\n",
    "\texons = sequence[\"exons\"]\n",
    "\n",
    "\tfor intron in introns:\n",
    "\t\tintrons_data.append(intron[\"data\"])\n",
    "\n",
    "\tfor exon in exons:\n",
    "\t\texons_data.append(exon[\"data\"])\n",
    "\n",
    "introns_data = list(set(introns_data))\n",
    "exons_data = list(set(exons_data))\n",
    "\n",
    "sequences = introns_data + exons_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sequences = []\n",
    "\n",
    "for sequence in sequences:\n",
    "\ttokenized_sequence = \"\"\n",
    "\tfor nucl in sequence:\n",
    "\t\ttokenized_sequence = tokenized_sequence + f\"[{nucl}]\"\n",
    "\n",
    "\ttokenized_sequences.append(tokenized_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_labels = [\"[INTRON]\" for _ in range(len(introns_data))] + [\"[EXON]\" for _ in range(len(exons_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = list(zip(tokenized_sequences, tokenized_labels))\n",
    "\n",
    "random.shuffle(all_data)\n",
    "\n",
    "sequences_shuffled, labels_shuffled = zip(*all_data)\n",
    "\n",
    "split_index = int(0.9 * len(sequences_shuffled))\n",
    "\n",
    "train_x = sequences_shuffled[:split_index]\n",
    "train_y = labels_shuffled[:split_index]\n",
    "\n",
    "test_x = sequences_shuffled[split_index:]\n",
    "test_y = labels_shuffled[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "max_length = int(np.percentile([len(seq) for seq in sequences], 95))\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50263, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = [\"[A]\", \"[C]\", \"[G]\", \"[T]\", \"[EXON]\", \"[INTRON]\"]\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SplicingSitesDataset(train_x, train_y, tokenizer, max_length=max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "train(model, dataloader, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(test_x)\n",
    "\n",
    "hits = 0\n",
    "for i in range(len(test_x)):\n",
    "\tsubject = test_x[i]\n",
    "\tlabel = test_y[i]\n",
    "\n",
    "\tprediction = predict(model, tokenizer, subject)\n",
    "\n",
    "\tif label == \"exon\":\n",
    "\t\tif prediction[:4] == \"exon\":\n",
    "\t\t\thits+=1\n",
    "\telif label == \"intron\":\n",
    "\t\tif prediction[:6] == \"intron\":\n",
    "\t\t\thits+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hits/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
