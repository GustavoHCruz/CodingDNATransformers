{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando apenas com os íntrons e éxons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./database/col_ac.mod1\", \"rb\")\n",
    "data = pickle.load(file)\n",
    "\n",
    "database = data[\"train\"] + data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "introns_data = []\n",
    "exons_data = []\n",
    "\n",
    "for sequence in database:\n",
    "\tintrons = sequence[\"introns\"]\n",
    "\texons = sequence[\"exons\"]\n",
    "\n",
    "\tfor intron in introns:\n",
    "\t\tintrons_data.append(intron[\"data\"])\n",
    "\n",
    "\tfor exon in exons:\n",
    "\t\texons_data.append(exon[\"data\"])\n",
    "\n",
    "introns_data = set(introns_data)\n",
    "exons_data = set(exons_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_input = []\n",
    "\n",
    "for sequence in introns_data:\n",
    "  transformers_input.append({\n",
    "    \"prompt\": f\"what is the classification for this sequence? {sequence}\",\n",
    "    \"completion\": \"[INTRON]\"\n",
    "  })\n",
    "\n",
    "for sequence in exons_data:\n",
    "  transformers_input.append({\n",
    "    \"prompt\": f\"what is the classification for this sequence? {sequence}\",\n",
    "    \"completion\": \"[EXON]\"\n",
    "  })\n",
    "\n",
    "random.shuffle(transformers_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proportion = 0.8\n",
    "dataset_len = len(transformers_input)\n",
    "crop = int(train_proportion * dataset_len)\n",
    "\n",
    "train = transformers_input[:crop]\n",
    "test = transformers_input[crop:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"A\", \"C\", \"G\", \"T\", \"[EXON]\", \"[INTRON]\"]\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_max_lengths = [len(tokenizer(seq[\"prompt\"])[\"input_ids\"]) for seq in transformers_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_length = int(np.percentile(sequence_max_lengths, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lenght for the sequences crop: {crop_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = Dataset.from_list(train)\n",
    "hf_test = Dataset.from_list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens(example):\n",
    "  inputs = tokenizer(example[\"prompt\"], truncation=True, padding=\"max_length\", max_length=crop_length)\n",
    "  outputs = tokenizer(example[\"completion\"], truncation=True, padding=\"max_length\", max_length=crop_length)\n",
    "  inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "  return inputs\n",
    "\n",
    "tokenized_train = hf_train.map(to_tokens, batched=True, batch_size=32)\n",
    "tokenized_test = hf_test.map(to_tokens, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./results\",\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  learning_rate=5e-5,\n",
    "  num_train_epochs=3,\n",
    "  per_device_train_batch_size=4,\n",
    "  save_steps=10,\n",
    "  save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_train,\n",
    "  eval_dataset=tokenized_test,\n",
    "  tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"spliceGPT\")\n",
    "tokenizer.save_pretrained(\"spliceGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 15\n",
    "prompt = \"Write a story about a dragon who learns to fly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "attention_mask = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,\n",
    "    num_beams=5,            # Usando busca por feixe para melhor desempenho\n",
    "    temperature=0.5,        # Experimente uma temperatura maior\n",
    "    top_k=50,               # Considera apenas os 50 tokens mais prováveis\n",
    "    top_p=0.95,             # Aplica amostragem com probabilidade acumulada\n",
    "    do_sample=False,         # Amostragem ativa\n",
    "    no_repeat_ngram_size=2  # Evita repetições de n-grams\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Write a story about a dragon who learns to fly.\n",
      "\n",
      "This is the story of a young girl who discovers that she can fly, but she has to learn how to do it in order to make it to the next level. The story begins with the girl's first encounter with a flying dragon, and it's up to her to find out what it is that makes her so special.\n"
     ]
    }
   ],
   "source": [
    "generated_sequence = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "print(\"Response:\", generated_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
