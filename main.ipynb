{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training only with introns and exons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./database/col_ac.mod1\", \"rb\")\n",
    "data = pickle.load(file)\n",
    "\n",
    "database = data[\"train\"] + data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "introns_data = []\n",
    "exons_data = []\n",
    "\n",
    "for sequence in database:\n",
    "\tintrons = sequence[\"introns\"]\n",
    "\texons = sequence[\"exons\"]\n",
    "\n",
    "\tfor intron in introns:\n",
    "\t\tintrons_data.append(intron[\"data\"])\n",
    "\n",
    "\tfor exon in exons:\n",
    "\t\texons_data.append(exon[\"data\"])\n",
    "\n",
    "introns_data = list(set(introns_data))\n",
    "exons_data = list(set(exons_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = introns_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_input = []\n",
    "idx = 0\n",
    "\n",
    "for sequence in introns_data:\n",
    "\ttokenized_sequence = \"\"\n",
    "\tfor nucl in sequence:\n",
    "\t\ttokenized_sequence = tokenized_sequence + f\"[{nucl}]\"\n",
    "\n",
    "\ttransformers_input.append({\n",
    "\t\t\"prompt\": f\"what is the classification for this sequence? {tokenized_sequence}\",\n",
    "\t\t\"completion\": \"[INTRON]\",\n",
    "\t\t\"idx\": idx\n",
    "\t})\n",
    "\tidx += 1\n",
    "\n",
    "for sequence in exons_data:\n",
    "\ttokenized_sequence = \"\"\n",
    "\tfor nucl in sequence:\n",
    "\t\ttokenized_sequence = tokenized_sequence + f\"[{nucl}]\"\n",
    "\n",
    "\ttransformers_input.append({\n",
    "\t\t\"prompt\": f\"what is the classification for this sequence? {tokenized_sequence}\",\n",
    "\t\t\"completion\": \"[EXON]\",\n",
    "\t\t\"idx\": idx\n",
    "\t})\n",
    "\tidx += 1\n",
    "\n",
    "random.shuffle(transformers_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proportion = 0.8\n",
    "dataset_len = len(transformers_input)\n",
    "crop = int(train_proportion * dataset_len)\n",
    "\n",
    "train = transformers_input[:crop]\n",
    "test = transformers_input[crop:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"gpt2\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[A]\", \"[C]\", \"[G]\", \"[T]\", \"[EXON]\", \"[INTRON]\"]\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_max_lengths = [len(tokenizer(seq[\"prompt\"])[\"input_ids\"]) for seq in transformers_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_length = int(np.percentile(sequence_max_lengths, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lenght for the sequences crop: {crop_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = Dataset.from_list(train)\n",
    "hf_test = Dataset.from_list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "  inputs = tokenizer(example[\"prompt\"], truncation=True, padding=\"max_length\", max_length=crop_length)\n",
    "  outputs = tokenizer(example[\"completion\"], truncation=True, padding=\"max_length\", max_length=crop_length)\n",
    "  inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "  return inputs\n",
    "\n",
    "tokenized_train = hf_train.map(tokenize_function, batched=True)\n",
    "tokenized_test = hf_test.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./results\",\n",
    "  eval_strategy=\"epoch\",\n",
    "  learning_rate=0.005,\n",
    "  num_train_epochs=20,\n",
    "  per_device_train_batch_size=16,\n",
    "  save_steps=50,\n",
    "  save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_train,\n",
    "  eval_dataset=tokenized_test,\n",
    "  processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"spliceGPT\")\n",
    "tokenizer.save_pretrained(\"spliceGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 15\n",
    "prompt = \"Write a story about a dragon who learns to fly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(\n",
    "    tokenized_prompt[\"input_ids\"].cuda(),\n",
    "    max_length=200,\n",
    "    num_beams=5,            # Usando busca por feixe para melhor desempenho\n",
    "    temperature=0.5,        # Experimente uma temperatura maior\n",
    "    top_k=50,               # Considera apenas os 50 tokens mais prováveis\n",
    "    top_p=0.95,             # Aplica amostragem com probabilidade acumulada\n",
    "    do_sample=False,         # Amostragem ativa\n",
    "    no_repeat_ngram_size=2  # Evita repetições de n-grams\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_prompt[\"input_ids\"])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequence = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "print(\"Response:\", generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
