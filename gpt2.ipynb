{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplicingSitesDataset(Dataset):\n",
    "\tdef __init__(self, sequences, labels, tokenizer, max_length):\n",
    "\t\tself.sequences = sequences\n",
    "\t\tself.labels = labels\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.max_length = max_length\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.sequences)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tprompt = self.sequences[idx]\n",
    "\t\tlabel = self.labels[idx]\n",
    "\n",
    "\t\tinput_text = f\"sequence: {prompt} awnser: \"\n",
    "\t\toutput_text = f\"{label}\"\n",
    "\t\tinput_ids = self.tokenizer.encode(input_text, truncation=True, max_length=self.max_length, add_special_tokens=True, padding=True)\n",
    "\t\tlabel_ids = self.tokenizer.encode(output_text, truncation=True, max_length=self.max_length, add_special_tokens=False)\n",
    "\n",
    "\t\tinput_ids += label_ids\n",
    "\t\tlabels = [-100] * len(input_ids[:-len(label_ids)]) + label_ids\n",
    "\n",
    "\t\treturn torch.tensor(input_ids), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\tinput_ids, labels = zip(*batch)\n",
    "\tmax_len = max(len(ids) for ids in input_ids)\n",
    "\tinput_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\tlabels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\treturn input_ids_padded, labels_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, epochs=3, device=\"cuda\"):\n",
    "\tmodel.train()\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor batch in dataloader:\n",
    "\t\t\tinput_ids, labels = [b.to(device) for b in batch]\n",
    "\t\t\toutputs = model(input_ids=input_ids, labels=labels)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\tprint(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, sequence, device=\"cuda\"):\n",
    "\tmodel.eval()\n",
    "\tinput_text = f\"sequence: {sequence} awnser: \"\n",
    "\tinput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = model.generate(\n",
    "\t\t\tinput_ids,\n",
    "\t\t\tmax_new_tokens=10,\n",
    "\t\t\trepetition_penalty=2.0,\n",
    "\t\t\ttop_k=50,\n",
    "\t\t\ttop_p=0.9,\n",
    "\t\t\tpad_token_id=tokenizer.eos_token_id,\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tcompletion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\t\treturn completion.replace(input_text, \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./database/col_ac.mod1\", \"rb\")\n",
    "data = pickle.load(file)\n",
    "\n",
    "database = data[\"train\"] + data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "introns_data = []\n",
    "exons_data = []\n",
    "\n",
    "for sequence in database:\n",
    "\tintrons = sequence[\"introns\"]\n",
    "\texons = sequence[\"exons\"]\n",
    "\n",
    "\tfor intron in introns:\n",
    "\t\tintrons_data.append(intron[\"data\"])\n",
    "\n",
    "\tfor exon in exons:\n",
    "\t\texons_data.append(exon[\"data\"])\n",
    "\n",
    "introns_data = list(set(introns_data))\n",
    "exons_data = list(set(exons_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = introns_data + exons_data\n",
    "labels = [\"intron\" for _ in range(len(introns_data))] + [\"exon\" for _ in range(len(exons_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "all_data = list(zip(sequences, labels))\n",
    "\n",
    "random.shuffle(all_data)\n",
    "\n",
    "sequences_shuffled, labels_shuffled = zip(*all_data)\n",
    "\n",
    "split_index = int(0.9 * len(sequences_shuffled))\n",
    "\n",
    "train_x = sequences_shuffled[:split_index]\n",
    "train_y = labels_shuffled[:split_index]\n",
    "\n",
    "test_x = sequences_shuffled[split_index:]\n",
    "test_y = labels_shuffled[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_length = int(np.percentile([len(seq) for seq in sequences], 95))\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[A]\", \"[C]\", \"[G]\", \"[T]\", \"[EXON]\", \"[INTRON]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SplicingSitesDataset(train_x, train_y, tokenizer, max_length=max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6825599449872971\n",
      "Epoch 2/10, Loss: 0.07910846535116434\n",
      "Epoch 3/10, Loss: 0.03698986181756481\n",
      "Epoch 4/10, Loss: 0.025260103847831488\n",
      "Epoch 5/10, Loss: 0.017396068065427244\n",
      "Epoch 6/10, Loss: 0.01097581772133708\n",
      "Epoch 7/10, Loss: 0.006646627892478136\n",
      "Epoch 8/10, Loss: 0.007252144568774384\n",
      "Epoch 9/10, Loss: 0.008844278592941918\n",
      "Epoch 10/10, Loss: 0.01107653695216868\n"
     ]
    }
   ],
   "source": [
    "train(model, dataloader, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "total = len(test_x)\n",
    "\n",
    "hits = 0\n",
    "for i in range(len(test_x)):\n",
    "\tsubject = test_x[i]\n",
    "\tlabel = test_y[i]\n",
    "\n",
    "\tprediction = predict(model, tokenizer, subject)\n",
    "\n",
    "\tif label == \"exon\":\n",
    "\t\tif prediction[:4] == \"exon\":\n",
    "\t\t\thits+=1\n",
    "\telif label == \"intron\":\n",
    "\t\tif prediction[:6] == \"intron\":\n",
    "\t\t\thits+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(hits/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
