{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import tokenize\n",
    "\n",
    "from math import ceil\n",
    "from typing import Any, Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25025aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CURRENT_DIR = Path().resolve()\n",
    "\n",
    "APPS_ROOT = CURRENT_DIR.parents[3]\n",
    "PROJECT_ROOT = CURRENT_DIR.parents[4]\n",
    "\n",
    "SHARED_DIR = APPS_ROOT / \"shared\"\n",
    "STORAGE_DIR = PROJECT_ROOT / \"storage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba475e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.optimization import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.models.auto.modeling_auto import AutoModelForCausalLM\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "\n",
    "def load_model(\n",
    "\tmodel_name: str\n",
    ") -> tuple[PreTrainedModel, PreTrainedTokenizerBase]:\n",
    "\tcheckpoint_path = os.path.join(STORAGE_DIR, \"models\", model_name)\n",
    "\n",
    "\tmodel = AutoModelForCausalLM.from_pretrained(checkpoint_path)\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "\treturn model, tokenizer\n",
    "\n",
    "def save_model(\n",
    "\tmodel_name: str,\n",
    "\tmodel: PreTrainedModel,\n",
    "\ttokenizer: PreTrainedTokenizerBase,\n",
    "\thistory: dict | None\n",
    ") -> None:\n",
    "\toutput_path = os.path.join(STORAGE_DIR, \"models\", model_name)\n",
    "\tmodel.save_pretrained(output_path)\n",
    "\ttokenizer.save_pretrained(output_path)\n",
    "\n",
    "\tif history:\n",
    "\t\tdf = pd.DataFrame(history)\n",
    "\t\tnow = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\t\tdf.to_csv(f\"{output_path}/history-{now}.csv\", index=False)\n",
    "\n",
    "def set_seed(seed) -> None:\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.use_deterministic_algorithms(True)\n",
    "\n",
    "\tos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\tos.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequence(sequence) -> str:\n",
    "\treturn f\"\".join(f\"[{nucl.upper()}]\" for nucl in sequence)\n",
    "\n",
    "def process_target(label) -> str:\n",
    "\treturn f\"[{label.upper()}]\"\n",
    "\n",
    "def promptfy(\n",
    "\tsequence: str,\n",
    "\torganism: str,\n",
    "\thide_prob: float,\n",
    "\tgene: str | None,\n",
    "\tflank_before: str | None,\n",
    "\tflank_after: str | None,\n",
    ") -> str:\n",
    "\toutput = f\"<|SEQUENCE|>{sequence}\\n\"\n",
    "\n",
    "\tif organism:\n",
    "\t\tif random.random() > hide_prob:\n",
    "\t\t\toutput += f\"<|ORGANISM|>{organism[:10]}\\n\"\n",
    "\n",
    "\tif gene:\n",
    "\t\tif random.random() > hide_prob:\n",
    "\t\t\toutput += f\"<|GENE|>{gene[:10]}\\n\"\n",
    "\t\n",
    "\tif flank_before:\n",
    "\t\tif random.random() > hide_prob:\n",
    "\t\t\toutput += f\"<|FLANK_BEFORE|>{flank_before}\\n\"\n",
    "\t\n",
    "\tif flank_after:\n",
    "\t\tif random.random() > hide_prob:\n",
    "\t\t\toutput += f\"<|FLANK_AFTER|>{flank_after}\\n\"\n",
    "\t\n",
    "\toutput += \"<|TARGET|>\"\n",
    "\n",
    "\treturn output\n",
    "\n",
    "class DNADatasetFinetune(IterableDataset):\n",
    "\t\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\tcsv_path: str,\n",
    "\t\t\ttokenizer,\n",
    "\t\t\tdataset_total_length: int,\n",
    "\t\t\tfeat_hide_prob: float,\n",
    "\t\t\tflanks_size: int = 25,\n",
    "\t\t\tsequence_max_length: int = 512,\n",
    "\t\t) -> None:\n",
    "\t\t\tself.csv_path = csv_path\n",
    "\t\t\tself.tokenizer = tokenizer\n",
    "\t\t\tself.max_length = sequence_max_length + flanks_size * 2 + 20\n",
    "\t\t\tself._length = dataset_total_length\n",
    "\t\t\tself.feat_hide_prob = feat_hide_prob\n",
    "\n",
    "\t\tdef __len__(self):\n",
    "\t\t\treturn self._length\n",
    "\t\t\n",
    "\t\tdef __iter__(self) -> Generator[dict[str, torch.Tensor], Any, None]:\n",
    "\t\t\twith open(self.csv_path, newline='') as csvfile:\n",
    "\t\t\t\treader = csv.DictReader(csvfile)\n",
    "\t\t\t\tfor row in reader:\n",
    "\t\t\t\t\tsequence = process_sequence(row[\"sequence\"])\n",
    "\t\t\t\t\ttarget = process_target(row[\"target\"])\n",
    "\t\t\t\t\torganism = row[\"organism\"]\n",
    "\t\t\t\t\tgene = row[\"gene\"]\n",
    "\t\t\t\t\tflank_before = row[\"flankBefore\"]\n",
    "\t\t\t\t\tflank_after = row[\"flankAfter\"]\n",
    "\n",
    "\t\t\t\t\tprompt = promptfy(\n",
    "\t\t\t\t\t\tsequence=sequence,\n",
    "\t\t\t\t\t\torganism=organism,\n",
    "\t\t\t\t\t\tgene=gene,\n",
    "\t\t\t\t\t\tflank_before=flank_before,\n",
    "\t\t\t\t\t\tflank_after=flank_after,\n",
    "\t\t\t\t\t\thide_prob=self.feat_hide_prob,\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\t\tprompt_encoded = self.tokenizer(\n",
    "\t\t\t\t\t\tprompt,\n",
    "\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\tpadding=\"max_length\",\n",
    "\t\t\t\t\t\tmax_length=self.max_length\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\t\tinput_ids = prompt_encoded[\"input_ids\"]\n",
    "\t\t\t\t\tattention_mask = prompt_encoded[\"attention_mask\"]\n",
    "\n",
    "\t\t\t\t\tyield {\n",
    "\t\t\t\t\t\t\"input_ids\": torch.tensor(input_ids),\n",
    "\t\t\t\t\t\t\"attention_mask\": torch.tensor(attention_mask),\n",
    "\t\t\t\t\t\t\"labels\": torch.tensor(self.tokenizer.encode(target))\n",
    "\t\t\t\t\t}\n",
    "\n",
    "class FinetuneDataCollator:\n",
    "\tdef __init__(self, tokenizer) -> None:\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.pad_token_id = tokenizer.pad_token_id\n",
    "\t\n",
    "\tdef __call__(self, batch) -> dict[str, torch.Tensor]:\n",
    "\t\tinput_ids = [example[\"input_ids\"] for example in batch]\n",
    "\t\tattention_mask = [example[\"attention_mask\"] for example in batch]\n",
    "\t\tlabels = [example[\"labels\"] for example in batch]\n",
    "\n",
    "\t\tinput_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "\t\tattention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "\t\tlabels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t\"input_ids\": input_ids_padded,\n",
    "\t\t\t\"attention_mask\": attention_mask_padded,\n",
    "\t\t\t\"labels\": labels_padded\n",
    "\t\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1323b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-exin\"\n",
    "seed = 1234\n",
    "uuid = \"uuid\"\n",
    "data_length = 30000\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19135291",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "model, tokenizer = load_model(model_name)\n",
    "\n",
    "data_path = os.path.join(SHARED_DIR, \"temp\", uuid)\n",
    "\n",
    "dataset = DNADatasetFinetune(\n",
    "\tcsv_path=data_path+\".csv\",\n",
    "\ttokenizer=tokenizer,\n",
    "\tdataset_total_length=data_length,\n",
    "\tfeat_hide_prob=0.0\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "\tdataset=dataset,\n",
    "\tbatch_size=batch_size,\n",
    "\tcollate_fn=FinetuneDataCollator(tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e446bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab510b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\tfor batch in tqdm(dataloader):\n",
    "\t\tinput_ids, attention_mask, label = [b.to(model.device) for b in batch.values()]\n",
    "\t\t\n",
    "\t\tresponses = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=1)\n",
    "\t\t\n",
    "\t\tfor response in responses:\n",
    "\t\t\ttokenizer.decode(response[0])\n",
    "\t\t\tif response[-1] == label[0][0]:\n",
    "\t\t\t\thit += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3688b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit/30000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
